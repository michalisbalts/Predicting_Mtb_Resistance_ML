{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f576db0a-e358-4546-8478-ba4fa1fbc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "abe4fa0a-81cb-476f-8337-c941bbc442d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a0e0a768-5643-444a-bf6c-9b65fc56b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold all data\n",
    "data_dict = {}\n",
    "\n",
    "# Define a set to keep track of all unique SNP positions\n",
    "snp_positions = set()\n",
    "\n",
    "# Define the path to the directory containing the edited VCF files\n",
    "edited_vcf_dir = r\"/mnt/raid0/michalis/snakemakeproject/edited_vcfs/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ee9c1-e6dd-4250-9650-a75f43a32c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edited_vcf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7fc1d424-036e-4375-86ca-390472f396a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_files =os.listdir(edited_vcf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bd93c-7589-48ce-98fb-c59c4011ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "49f76af3-bdb0-4050-9a06-691b37d5524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = vcf_files[0]\n",
    "unique_id = os.path.join(edited_vcf_dir,file_path)\n",
    "#print(unique_id)\n",
    "snp_positions=[]\n",
    "genotypes=[]\n",
    "with open(unique_id, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().replace('\\r', '')  # Strip newline and remove carriage return\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "        parts = line.split()\n",
    "        snp_position = int(parts[0])\n",
    "        genotype = parts[1]\n",
    "        snp_positions.append(snp_position)\n",
    "        genotypes.append(genotype)\n",
    "        #genotypes = [-1 if x == '.' else x for x in genotypes]\n",
    "        #genotypes = [int(x) for x in genotypes]\n",
    "df = pd.DataFrame({'SNPs' : snp_positions, file_path: genotypes})\n",
    "# Replace '.' with 0 in column 'A'\n",
    "df[file_path] = df[file_path].replace('.', 0)\n",
    "#df[file_path] = df[file_path].replace({val: 0 for val in df[file_path].unique() if val not in [0, 1]})\n",
    "# Convert values in column 'A' to integers\n",
    "df[file_path] = df[file_path].astype(int)\n",
    "df[file_path] = df[file_path].replace({val: 1 for val in df[file_path].unique() if val not in [0, 1]})\n",
    "sdf = df.astype(pd.SparseDtype(int, fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319cce3-c80a-4fe3-af20-23d84ec3ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad01e3-e330-4caa-a195-87e7ed13164e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for file_path in vcf_files[1:12261]:\n",
    "    unique_id = os.path.join(edited_vcf_dir,file_path)\n",
    "    #print(unique_id)\n",
    "    count+=1\n",
    "    snp_positions=[]\n",
    "    genotypes=[]\n",
    "    print(count)\n",
    "    with open(unique_id, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().replace('\\r', '')  # Strip newline and remove carriage return\n",
    "            if not line:\n",
    "                continue  # skip empty lines\n",
    "            parts = line.split()\n",
    "            genotype = parts[1]\n",
    "            genotypes.append(genotype)\n",
    "            #genotypes = [-1 if x == '.' else x for x in genotypes]\n",
    "            #genotypes = [int(x) for x in genotypes]\n",
    "    sdf[file_path] = genotypes\n",
    "    # Replace '.' with -1 in column 'A'\n",
    "    # Replace all values except 0 or 1 with 0\n",
    "    sdf[file_path] = sdf[file_path].replace('.', 0)\n",
    "    # Convert values in column 'A' to integers\n",
    "    sdf[file_path] = sdf[file_path].astype(int)\n",
    "    # Replace all values except 0 or 1 with 0\n",
    "    sdf[file_path] = sdf[file_path].replace({val: 1 for val in sdf[file_path].unique() if val not in [0, 1]})\n",
    "\n",
    "    \n",
    "\n",
    "    sdf = sdf.astype(pd.SparseDtype(int, fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e699f-b076-4e8d-84c0-a92f6dd1d901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881ba2f-ca1d-404d-9de8-4e5c75cd5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "# Define chunk size \n",
    "chunk_size = 1000\n",
    "\n",
    "# Initialize a list to store chunks of the sparse matrix\n",
    "array_ML_chunks = []\n",
    "\n",
    "# Iterate over columns in chunks, starting from column 1 (index 0 is skipped)\n",
    "for start_col in range(1, sdf.shape[1], chunk_size):  # Process all columns of sdf\n",
    "    end_col = min(start_col + chunk_size, sdf.shape[1])\n",
    "\n",
    "    # Select and transpose the current chunk\n",
    "    df_transposed_chunk = sdf.iloc[:, start_col:end_col].T\n",
    "\n",
    "    # Convert chunk to sparse matrix\n",
    "    array_ML_chunk = csr_matrix(df_transposed_chunk.values)\n",
    "\n",
    "    # Append the sparse matrix chunk to the list\n",
    "    array_ML_chunks.append(array_ML_chunk)\n",
    "\n",
    "    # Print a progress message after processing each chunk\n",
    "    print(f\"Processed chunk {len(array_ML_chunks)} (columns {start_col}-{end_col - 1})\")\n",
    "\n",
    "# Combine chunks into a single sparse matrix\n",
    "array_ML_sparse = vstack(array_ML_chunks, format='csr')\n",
    "\n",
    "# Now array_ML.shape should be (12261, 1265713)\n",
    "print(array_ML_sparse .shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d4d59-012f-419e-a181-75cb5ab80183",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_ML_sparse .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e028c2-6d36-4a63-a200-c54c25275d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Read the sdf file which is assumed to be already loaded into a variable 'sdf'\n",
    "columns_sdf = sdf.iloc[:, 1:12262].columns\n",
    "columns_sdf = np.array([s[:-4] for s in columns_sdf])\n",
    "\n",
    "# Assuming array_ML_sparse is your sparse genotype data matrix\n",
    "print(f\"Original shape of array_ML_sparse: {array_ML_sparse.shape}\")\n",
    "\n",
    "# Sum the columns to find columns with only zeros\n",
    "column_sums = np.array(array_ML_sparse.sum(axis=0)).flatten()\n",
    "\n",
    "# Identify columns that have at least one non-zero entry\n",
    "non_zero_columns = column_sums > 0\n",
    "\n",
    "# Filter the columns in array_ML_sparse\n",
    "array_ML_sparse_filtered = array_ML_sparse[:, non_zero_columns]\n",
    "\n",
    "print(f\"New shape of array_ML_sparse: {array_ML_sparse_filtered.shape}\")\n",
    "\n",
    "# Function to process the data for each antibiotic\n",
    "def process_data(metadata_file, phenotype_column):\n",
    "    # Read the phenotypes file \n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "\n",
    "    # Check if all IDs in columns_sdf are in metadata['UNIQUEID']\n",
    "    if not set(columns_sdf).issubset(set(metadata['UNIQUEID'])):\n",
    "        raise ValueError(\"Not all patient IDs in the feature matrix are present in the metadata file.\")\n",
    "\n",
    "    # Extract target values based on UNIQUEID\n",
    "    phenotype_values = metadata.set_index('UNIQUEID').loc[columns_sdf, phenotype_column]\n",
    "\n",
    "    # Convert the extracted phenotype_values to a numpy array\n",
    "    phenotype_values = phenotype_values.values\n",
    "\n",
    "    # Convert array to str\n",
    "    phenotype_values = phenotype_values.astype('str')\n",
    "\n",
    "    # Find indices where values are 'S' or 'R'\n",
    "    valid_indices = (phenotype_values == 'S') | (phenotype_values == 'R')\n",
    "\n",
    "    # Filter out 'I' and NaN values\n",
    "    y = phenotype_values[valid_indices]\n",
    "    X = array_ML_sparse_filtered[valid_indices]\n",
    "\n",
    "    # Replace 'S' with 0 and 'R' with 1\n",
    "    y[y == 'S'] = 0\n",
    "    y[y == 'R'] = 1\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Process data for each antibiotic\n",
    "X_emb, y_emb = process_data(\"Cryptic_Ethambutol_Filtered.csv\", 'EMB_BINARY_PHENOTYPE')\n",
    "X_inh, y_inh = process_data(\"Cryptic_Isoniazid_Filtered.csv\", 'INH_BINARY_PHENOTYPE')\n",
    "X_rif, y_rif = process_data(\"Cryptic_Rifampicin_Filtered.csv\", 'RIF_BINARY_PHENOTYPE')\n",
    "\n",
    "# The resulting X_emb, y_emb, X_inh, y_inh, X_rif, and y_rif are now filtered and converted correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b623f20-bced-4ce8-b0fd-fa35d820acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_rif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841d5ce-d523-469f-8129-e2b12a34795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41f1c0-5a7b-4aa8-9615-44cfe4fe303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_inh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50c83c-9114-457e-a5aa-155b4e19be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144e337-fa5f-4b42-a27a-0afcb57d9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_inh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb58292-af7b-4790-b915-416e57104ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_rif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ab79b-b161-4d59-bee1-491ac55b9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Assuming X_inh, X_emb, and X_rif are your filtered genotype data matrices in CSR format\n",
    "\n",
    "# Variance thresholding function\n",
    "def variance_threshold_reduction(X, threshold=0.01):\n",
    "    print(f\"Original shape of X: {X.shape}\")\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    X_reduced = selector.fit_transform(X)\n",
    "    print(f\"Shape of X after variance thresholding: {X_reduced.shape}\")\n",
    "    return X_reduced\n",
    "\n",
    "# Apply the variance thresholding to X_inh\n",
    "X_inh_reduced = variance_threshold_reduction(X_inh, threshold=0.01)\n",
    "\n",
    "# Apply the variance thresholding to X_emb\n",
    "X_emb_reduced = variance_threshold_reduction(X_emb, threshold=0.01)\n",
    "\n",
    "# Apply the variance thresholding to X_rif\n",
    "X_rif_reduced = variance_threshold_reduction(X_rif, threshold=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8986b8-7401-4682-82ca-9548270351d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_inh_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24679fb1-b84e-46db-b3de-dc2224ba5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_emb_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a77c0b-48fc-456e-8fbb-d42f03a30c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_rif_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "47b2a330-933e-45da-af1e-f28c758e25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a667e21-37f0-4b3b-b712-875fddce2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_inh, y_inh, X_emb, y_emb, X_rif, y_rif are already defined and loaded\n",
    "# Assuming X_inh_reduced, X_emb_reduced, X_rif_reduced are already defined\n",
    "\n",
    "# Function to train and evaluate SVM with different hyperparameters\n",
    "def evaluate_svm(X, y):\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'C': {0.1, 1, 10},\n",
    "        'kernel': {'linear', 'rbf'}\n",
    "    }\n",
    "\n",
    "    # Initialize SVM classifier \n",
    "    svm_model = SVC(probability=True, random_state=1, verbose=False)\n",
    "\n",
    "    # Initialize GridSearchCV with reduced n_jobs\n",
    "    grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=0)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_svm_model.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    # Print test set metrics\n",
    "    print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "    print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "    print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "    print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "# List of datasets and their corresponding labels\n",
    "datasets = [\n",
    "    (X_inh, y_inh, \"X_inh\"),\n",
    "    (X_emb, y_emb, \"X_emb\"),\n",
    "    (X_rif, y_rif, \"X_rif\"),\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Evaluate SVM with different C values for each dataset\n",
    "C_values = [1]\n",
    "for X, y, label in datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "    for C in C_values:\n",
    "        print(f\"Evaluating SVM with C={C}\")\n",
    "        evaluate_svm(X, y, C)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cbc1fe7-3fdc-4f31-9fed-f9d5c93966c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_value =1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4beac-ab61-4e46-8f4c-5451ba907bae",
   "metadata": {},
   "source": [
    "# SVM INH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c29ce-0903-4630-afe3-d7622d4e5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier with Linear kernel for faster computation\n",
    "svm_inh = SVC(kernel='linear', C=1, probability=True, random_state=1, verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "svm_inh.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_inh.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def sensitivity_specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59446c87-b36d-4bd5-a185-2062aed88cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = svm_inh.predict(X_test)\n",
    "y_pred_prob = svm_inh.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/svm/inh/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/svm/inh/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/svm/inh/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/inh/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/inh/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885b96f-b540-4c25-a8e9-f943d3132f3e",
   "metadata": {},
   "source": [
    "# SVM emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21692e1d-c242-4939-a578-a0fae7a835c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier with Linear kernel for faster computation\n",
    "svm_emb = SVC(kernel='linear', C=1, probability=True, random_state=1, verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "svm_emb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_emb.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def sensitivity_specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a810b4-19de-4cfa-82b6-8dad21d78465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = svm_emb.predict(X_test)\n",
    "y_pred_prob = svm_emb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/svm/emb/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/svm/emb/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/svm/emb/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/emb/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/emb/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a988b3c-68d6-408f-991b-7d4d1b9379c4",
   "metadata": {},
   "source": [
    "# SVM rif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b967564-74de-437e-bd7a-4558d737f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier with Linear kernel for faster computation\n",
    "svm_rif = SVC(kernel='linear', C=1, probability=True, random_state=1, verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "svm_rif.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_rif.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def sensitivity_specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec7c9d-3d42-40ff-8508-6502f374a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = svm_rif.predict(X_test)\n",
    "y_pred_prob = svm_rif.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/svm/rif/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/svm/rif/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/svm/rif/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/rif/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/rif/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6cec2-0cf8-42db-84ce-60d350e5d077",
   "metadata": {},
   "source": [
    "# SVM inh r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4b16d-2edf-4e4b-95d5-e7ead6499a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh_reduced, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier with Linear kernel for faster computation\n",
    "svm_inh_reduced = SVC(kernel='linear', C=1, probability=True, random_state=1, verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "svm_inh_reduced.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_inh_reduced.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def sensitivity_specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2183eb1-49e8-4d59-9c16-ad2c2c4a7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh_reduced, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = svm_inh_reduced.predict(X_test)\n",
    "y_pred_prob = svm_inh_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/svm/inh_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/svm/inh_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/svm/inh_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/inh_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/inh_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193489a-90c4-4ee3-b70b-cbe94f707406",
   "metadata": {},
   "source": [
    "# SVM emb r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcbe92-218b-4884-a81f-954c76479c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb_reduced, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier with Linear kernel for faster computation\n",
    "svm_emb_reduced = SVC(kernel='linear', C=1, probability=True, random_state=1, verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "svm_emb_reduced.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_emb_reduced.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def sensitivity_specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d469d3-0386-4c47-94f3-e21b8829590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb_reduced, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = svm_emb_reduced.predict(X_test)\n",
    "y_pred_prob = svm_emb_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/svm/emb_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/svm/emb_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/svm/emb_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/emb_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/emb_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad3edd-0cb6-4834-ac39-d620c2677e33",
   "metadata": {},
   "source": [
    "# SVM rif r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c5962-8d8b-444e-9865-f4c90ae0dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif_reduced, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier with Linear kernel for faster computation\n",
    "svm_rif_reduced = SVC(kernel='linear', C=1, probability=True, random_state=1, verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "svm_rif_reduced.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_rif_reduced.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "def sensitivity_specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Set Accuracy (C={C_value}):\", accuracy_score(y_test, predictions))\n",
    "print(f\"Test Set F1-Score (C={C_value}):\", f1_score(y_test, predictions))\n",
    "print(f\"Test Set ROC-AUC (C={C_value}):\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "print(f\"Test Set Sensitivity (C={C_value}): {sensitivity:.3f}\")\n",
    "print(f\"Test Set Specificity (C={C_value}): {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05266c0-fa8c-44f2-8d4f-9395c07ba636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif_reduced, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = svm_rif_reduced.predict(X_test)\n",
    "y_pred_prob = svm_rif_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/svm/rif_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/svm/rif_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/svm/rif_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/rif_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/svm/rif_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43977a-3a36-45b6-9198-2f0c969c8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF WITH HYPERPARAMETER TUNING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a182a9-3f93-4cd4-bdc8-6a22ce676592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_inh, y_inh, X_emb, y_emb, X_rif, y_rif are already defined and loaded\n",
    "# Assuming X_inh_reduced, X_emb_reduced, X_rif_reduced are already defined\n",
    "\n",
    "# List of datasets and their corresponding labels\n",
    "datasets = [\n",
    "    (X_inh, y_inh, \"X_inh\"),\n",
    "    (X_emb, y_emb, \"X_emb\"),\n",
    "    (X_rif, y_rif, \"X_rif\"),\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Dictionary to store the best models for each dataset\n",
    "best_models = {}\n",
    "\n",
    "for X, y, label in datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Initialize Random Forest classifier\n",
    "    rf_model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "    # Initialize GridSearchCV with reduced n_jobs\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=0)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the best model in the dictionary\n",
    "    best_models[label] = best_rf_model\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_rf_model.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    # Print test set metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, predictions))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# best_models dictionary now contains the best model for each dataset\n",
    "for label, model in best_models.items():\n",
    "    print(f\"Best model for {label} is stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "543b1907-6dc4-4b39-a38c-8a98869a4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_inh = best_models['X_inh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af229c4-e323-4f47-9d1e-672019d011a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = rf_inh.predict(X_test)\n",
    "y_pred_prob = rf_inh.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/rf/inh/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/rf/inh/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/rf/inh/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/inh/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/inh/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1f53667-0b86-41e7-a4e2-6fc45267fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_emb = best_models['X_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf7729-7b48-4ec7-b5e2-835954e4c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = rf_emb.predict(X_test)\n",
    "y_pred_prob = rf_emb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/rf/emb/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/rf/emb/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/rf/emb/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/emb/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/emb/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "214dfe54-5733-45ef-8ed9-1ba8246715c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rif = best_models['X_rif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97608684-39d4-49d1-aeae-b4cd7b6a3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = rf_rif.predict(X_test)\n",
    "y_pred_prob = rf_rif.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/rf/rif/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/rf/rif/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/rf/rif/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/rif/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/rif/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "303a4ea1-86c4-4188-b825-4a1b16c7cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_inh_reduced = best_models['X_inh_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed6824-d915-47c9-a2f6-69c0a3ac420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_inh_reduced, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = rf_inh_reduced.predict(X_test)\n",
    "y_pred_prob = rf_inh_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/rf/inh_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/rf/inh_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/rf/inh_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/inh_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/inh_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc12a38e-e75b-4e14-8b0f-2a8860d167a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_emb_reduced = best_models['X_emb_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61563a1e-5394-4472-be1c-da52b65d3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb_reduced, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = rf_emb_reduced.predict(X_test)\n",
    "y_pred_prob = rf_emb_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/rf/emb_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/rf/emb_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/rf/emb_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/emb_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/emb_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "176b1461-8f6a-477a-b495-eda695da591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rif_reduced = best_models['X_rif_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3d5f9-f677-42b5-acb1-32345d14539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif_reduced, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = rf_rif_reduced.predict(X_test)\n",
    "y_pred_prob = rf_rif_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/rf/rif_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/rf/rif_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/rf/rif_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/rif_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/rf/rif_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d936096-2758-499a-bf88-113be81fac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION WITH L2 REGULARIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106821c2-0282-4c6c-9a81-1a60fd23f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer, confusion_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "best_modelslr = {}\n",
    "\n",
    "# Function to perform feature selection and evaluate Logistic Regression for a given dataset\n",
    "def evaluate_log_reg_with_feature_selection(X, y):\n",
    "    # Feature selection or dimensionality reduction (example: selecting top 1000 features)\n",
    "    if not sparse.issparse(X):\n",
    "        selector = SelectKBest(chi2, k=1000)\n",
    "        X = selector.fit_transform(X, y)\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    if sparse.issparse(X):\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'solver': {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},\n",
    "        'penalty': {None, 'l1', 'l2', 'elasticnet'},\n",
    "        'C': {100, 10, 1.0, 0.1, 0.01}\n",
    "    }\n",
    "\n",
    "    # Initialize Logistic Regression with L2 regularization and smaller max_iter\n",
    "    lr = LogisticRegression(prandom_state=1)\n",
    "\n",
    "    # Initialize GridSearchCV with reduced n_jobs\n",
    "    grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=4, verbose=0)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    log_reg = grid_search.best_estimator_\n",
    "\n",
    "    best_modelslr{label} = log_reg\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = log_reg.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "\n",
    "    # Print test set metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, y_pred)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "# List of datasets and their corresponding labels\n",
    "datasets = [\n",
    "    (X_inh, y_inh, \"X_inh\"),\n",
    "    (X_emb, y_emb, \"X_emb\"),\n",
    "    (X_rif, y_rif, \"X_rif\"),\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Evaluate Logistic Regression with feature selection for each dataset\n",
    "for X, y, label in datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "    evaluate_log_reg_with_feature_selection(X, y)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ce7341ed-0f4e-46c9-8cc0-997de69bba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_inh_reduced = best_modelslr['X_inh_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161cabb-f26f-4d38-b81c-4c136a9e76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_inh_reduced, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = lr_inh_reduced.predict(X_test)\n",
    "y_pred_prob = lr_inh_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/lr/inh_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/lr/inh_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/lr/inh_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/lr/inh_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/lr/inh_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70692c8a-08b0-4110-8c3a-5f502727a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_emb_reduced = best_modelslr['X_emb_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2ff21-d142-4a32-864a-86a50cd8e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb_reduced, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = lr_emb_reduced.predict(X_test)\n",
    "y_pred_prob = lr_emb_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/lr/emb_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/lr/emb_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/lr/emb_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/lr/emb_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/lr/emb_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48374db-7126-4ca0-86a4-32831fdea6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST WITH HYPERPARAMETER TUNING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6d58e1c9-71e5-4d91-9c3f-e4aa034cf406",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rif_reduced = best_modelslr['X_rif_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35afe49-28eb-46b0-a50b-cd5de070c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_rif_reduced, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(with_mean=False) if sparse.issparse(X) else StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = lr_rif_reduced.predict(X_test)\n",
    "y_pred_prob = lr_rif_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/lr/rif_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/lr/rif_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/lr/rif_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/lr/rif_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/lr/rif_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df272ca-ab9f-464b-bef9-637c20542ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_inh, y_inh, X_emb, y_emb, X_rif, y_rif are already defined and loaded\n",
    "# Assuming X_inh_reduced, X_emb_reduced, X_rif_reduced are already defined\n",
    "\n",
    "# List of datasets and their corresponding labels\n",
    "datasets = [\n",
    "    (X_inh, y_inh, \"X_inh\"),\n",
    "    (X_emb, y_emb, \"X_emb\"),\n",
    "    (X_rif, y_rif, \"X_rif\"),\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Dictionary to store the best models for each dataset\n",
    "best_modelsxg = {}\n",
    "\n",
    "for X, y, label in datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Define the initial XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',  \n",
    "        'random_state': 1         \n",
    "    }\n",
    "\n",
    "    # Define the parameter grid to search\n",
    "    param_distributions = {\n",
    "        'eta': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'n_estimators': [50, 100, 200]\n",
    "    }\n",
    "\n",
    "    # Initialize XGBClassifier with initial params\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Create the scoring dictionary\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score), \n",
    "        'f1': make_scorer(f1_score), \n",
    "        'roc_auc': make_scorer(roc_auc_score)\n",
    "    }\n",
    "\n",
    "    # Use RandomizedSearchCV to find the best parameters\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, scoring='roc_auc', cv=5, n_jobs=4, n_iter=10, verbose=0, random_state=1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"Best Parameters found: \", best_params)\n",
    "\n",
    "    # Initialize XGBClassifier with best parameters\n",
    "    best_model = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "    # Fit the model with the best parameters\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Store the best model in the dictionary\n",
    "    best_modelsxg[label] = best_model\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict_proba(X_test)[:, 1]  # Get probabilities for positive class\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    # Print test set metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, predictions))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, predictions)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# best_models dictionary now contains the best model for each dataset\n",
    "for label, model in best_modelsxg.items():\n",
    "    print(f\"Best model for {label} is stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb523003-5f29-4ba9-a4ba-64e4752ed9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_inh = best_modelsxg['X_inh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9cf44-70a8-422d-9e50-5e5dd2ba3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = xg_inh.predict(X_test)\n",
    "y_pred_prob = xg_inh.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/xg/inh/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/xg/inh/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/xg/inh/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/inh/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/inh/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5ff34ca-8272-40ef-93b8-874163f8106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_emb = best_modelsxg['X_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e356f5-234c-49db-b5f8-8481000940e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = xg_emb.predict(X_test)\n",
    "y_pred_prob = xg_emb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/xg/emb/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/xg/emb/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/xg/emb/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/emb/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/emb/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbf89f85-b258-4cda-8961-27c7920ecf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_rif = best_modelsxg['X_rif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e989f-a71c-4ed8-8953-968acd1e6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = xg_rif.predict(X_test)\n",
    "y_pred_prob = xg_rif.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/xg/rif/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/xg/rif/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/xg/rif/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/rif/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/rif/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "feea6928-ae20-4757-81d4-e028e2b71eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_inh_reduced = best_modelsxg['X_inh_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3f088-2b41-42be-882a-7961ab09c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh_reduced, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = xg_inh_reduced.predict(X_test)\n",
    "y_pred_prob = xg_inh_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/xg/inh_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/xg/inh_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/xg/inh_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/inh_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/inh_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a93d8d5-4d07-4ac7-b121-311c2a7da577",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_emb_reduced = best_modelsxg['X_emb_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed946d45-b77f-4f34-a298-de03e3f5707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb_reduced, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = xg_emb_reduced.predict(X_test)\n",
    "y_pred_prob = xg_emb_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/xg/emb_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/xg/emb_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/xg/emb_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/emb_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/emb_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d978470e-bcb4-4303-a50a-e2ce476da58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_rif_reduced = best_modelsxg['X_rif_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4085db-1046-4ebb-9be8-5cffac493771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif_reduced, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = xg_rif_reduced.predict(X_test)\n",
    "y_pred_prob = xg_rif_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/xg/rif_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/xg/rif_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/xg/rif_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/rif_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xg/rif_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63fe9d-4419-4c61-8d9d-62a8f8e08072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4027bc6-0b0b-417f-a8ea-d6597279b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from scipy import sparse\n",
    "from keras import backend as K\n",
    "\n",
    "# Function to train and evaluate a CNN for a given dataset\n",
    "def evaluate_cnn(X, y):\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    if sparse.issparse(X):\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "    # Reshape data for Conv1D layer\n",
    "    X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    # Define the CNN model using Input layer\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "    # Train the model with a smaller batch size\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, \n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, y_pred)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "    # Clear the Keras session to free up memory\n",
    "    K.clear_session()\n",
    "\n",
    "# List of reduced datasets and their corresponding labels\n",
    "reduced_datasets = [\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Evaluate CNN for each reduced dataset\n",
    "for X, y, label in reduced_datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "    evaluate_cnn(X, y)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92a2d9-af6e-42e4-8c66-25fad2788d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from scipy import sparse\n",
    "from keras import backend as K\n",
    "\n",
    "# Function to train and evaluate a CNN for a given dataset\n",
    "def evaluate_cnn(X, y):\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    if sparse.issparse(X):\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "    # Reshape data for Conv1D layer\n",
    "    X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    # Define the CNN model using Input layer\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "    # Train the model with a smaller batch size\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, \n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, y_pred)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "    # Clear the Keras session to free up memory\n",
    "    K.clear_session()\n",
    "\n",
    "# List of reduced datasets and their corresponding labels\n",
    "reduced_datasets = [\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Evaluate CNN for each reduced dataset\n",
    "for X, y, label in reduced_datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "    evaluate_cnn(X, y)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b088d-e806-4923-97c5-88cf60dd6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from scipy import sparse\n",
    "from keras import backend as K\n",
    "\n",
    "# Function to train and evaluate a CNN for a given dataset\n",
    "def evaluate_cnn(X, y):\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    if sparse.issparse(X):\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "    # Reshape data for Conv1D layer\n",
    "    X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    # Define the CNN model using Input layer\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "    # Train the model with a smaller batch size\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, \n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, y_pred)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "    # Clear the Keras session to free up memory\n",
    "    K.clear_session()\n",
    "\n",
    "# List of reduced datasets and their corresponding labels\n",
    "reduced_datasets = [\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Evaluate CNN for each reduced dataset\n",
    "for X, y, label in reduced_datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "    evaluate_cnn(X, y)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e0b74-2033-403a-a8ae-ad1af5bccef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from scipy import sparse\n",
    "from keras import backend as K\n",
    "\n",
    "# Assuming X_inh_reduced, y_inh, X_emb_reduced, y_emb, X_rif_reduced, y_rif are already defined and loaded\n",
    "\n",
    "# List of reduced datasets and their corresponding labels\n",
    "reduced_datasets = [\n",
    "    (X_inh_reduced, y_inh, \"X_inh_reduced\"),\n",
    "    (X_emb_reduced, y_emb, \"X_emb_reduced\"),\n",
    "    (X_rif_reduced, y_rif, \"X_rif_reduced\")\n",
    "]\n",
    "\n",
    "# Dictionary to store the trained models for each dataset\n",
    "trained_models = {}\n",
    "\n",
    "for X, y, label in reduced_datasets:\n",
    "    print(f\"Evaluating dataset: {label}\")\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "    # Feature scaling\n",
    "    if sparse.issparse(X):\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "    # Reshape data for Conv1D layer\n",
    "    X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    # Define the CNN model using Input layer\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "    # Train the model with a smaller batch size\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, \n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    trained_models[label] = model\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Test Set F1-Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Test Set ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    def sensitivity_specificity_score(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        return sensitivity, specificity\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity_score(y_test, y_pred)\n",
    "    print(f\"Test Set Sensitivity: {sensitivity:.3f}\")\n",
    "    print(f\"Test Set Specificity: {specificity:.3f}\")\n",
    "\n",
    "    # Clear the Keras session to free up memory\n",
    "    K.clear_session()\n",
    "    print(\"\\n\")\n",
    "\n",
    "# trained_models dictionary now contains the trained model for each dataset\n",
    "for label, model in trained_models.items():\n",
    "    print(f\"Trained model for {label} is stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8ebfd10-6e62-47b3-a29d-f584f05f2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_inh = trained_models['X_inh_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c464b4-1741-474c-a6f5-db0072257d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inh_reduced, y_inh, test_size=0.2, random_state=1, stratify=y_inh)\n",
    "\n",
    "# Feature scaling\n",
    "if sparse.issparse(X):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for Conv1D layer\n",
    "X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = cnn_inh.predict(X_test).flatten()\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/cnn/inh_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/cnn/inh_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/cnn/inh_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/cnn/inh_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/cnn/inh_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cf4f4cda-e7f2-47f2-a591-1b9b0229f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_emb = trained_models['X_emb_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16587d22-8202-4311-be1b-d36be577fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_emb_reduced, y_emb, test_size=0.2, random_state=1, stratify=y_emb)\n",
    "\n",
    "# Feature scaling\n",
    "if sparse.issparse(X):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for Conv1D layer\n",
    "X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = cnn_emb.predict(X_test).flatten()\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/cnn/emb_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/cnn/emb_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/cnn/emb_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/cnn/emb_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/cnn/emb_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "04d6d6ff-a7e5-4552-ac52-ad6554f822c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rif = trained_models['X_rif_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974896b-2b3a-4935-97ef-6ba24a2cd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, recall_score, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rif_reduced, y_rif, test_size=0.2, random_state=1, stratify=y_rif)\n",
    "\n",
    "# Feature scaling\n",
    "if sparse.issparse(X):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train).toarray() if sparse.issparse(X_train) else scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test).toarray() if sparse.issparse(X_test) else scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for Conv1D layer\n",
    "X_train = X_train[..., np.newaxis]  # Add a new axis for the single channel\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = cnn_rif.predict(X_test).flatten()\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=['S', 'R'], yticklabels=['S', 'R'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('plots/cnn/rif_r/cm.png')\n",
    "plt.close()\n",
    "\n",
    "# Classification Report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Replace '0' and '1' with 'S' and 'R'\n",
    "cr_df.index = cr_df.index.map({'0': 'S', '1': 'R', 'accuracy': 'accuracy', 'macro avg': 'macro avg', 'weighted avg': 'weighted avg'})\n",
    "\n",
    "# Plot and save Classification Report as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cr_df.iloc[:-1, :-1], annot=True, cmap='Blues', xticklabels=cr_df.columns[:-1], yticklabels=cr_df.index[:-1])\n",
    "plt.title('Classification Report')\n",
    "plt.savefig('plots/cnn/rif_r/cr.png')\n",
    "plt.close()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot and save ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='green', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('plots/cnn/rif_r/roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot Recall and Accuracy Scores against probability thresholds\n",
    "thresholds = np.arange(0.0, 1.1, 0.1)\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    recalls.append(recall_score(y_test, y_pred_threshold))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_threshold))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green', linestyle='-')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='blue', linestyle='-')\n",
    "plt.axvline(x=0.1, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Recall and Accuracy Scores')\n",
    "plt.legend()\n",
    "plt.savefig('plots/cnn/rif_r/recall_accuracy_thresholds.png')\n",
    "plt.close()\n",
    "\n",
    "# Density plot for predicted probability distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_pred_prob[y_test == 0], label='Susceptible', shade=True, color='green')\n",
    "sns.kdeplot(y_pred_prob[y_test == 1], label='Resistant', shade=True, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Probability Distributions')\n",
    "plt.legend()\n",
    "plt.savefig('plots/cnn/rif_r/predicted_probability_distributions.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44cffc-b542-468c-9f64-4019bedbdd00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
